{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-21 21:28:02,710\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-01-21 21:28:02,856\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "#torch and lightning (deep learning/model creation and training)\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "#ray (tune/grid search)\n",
    "from ray import train, tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "#matplotlib and seaborn (plotting)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# #pyOD (outlier detection)\n",
    "# from pyod.models.iforest import IForest\n",
    "\n",
    "#mlflow (loggind/tracking)\n",
    "import mlflow\n",
    "\n",
    "#general purpose\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X86_LOCAL_APIC</th>\n",
       "      <th>OPENVSWITCH</th>\n",
       "      <th>TEXTSEARCH_FSM</th>\n",
       "      <th>NETFILTER_XT_MATCH_TCPMSS</th>\n",
       "      <th>MPLS</th>\n",
       "      <th>NFC_HCI</th>\n",
       "      <th>NETFILTER_XT_MATCH_TIME</th>\n",
       "      <th>NET_MPLS_GSO</th>\n",
       "      <th>NFC_SHDLC</th>\n",
       "      <th>NETFILTER_XT_MATCH_U32</th>\n",
       "      <th>...</th>\n",
       "      <th>ARCH_SUPPORTS_INT128</th>\n",
       "      <th>SLABINFO</th>\n",
       "      <th>MICROCODE_AMD</th>\n",
       "      <th>ISDN_DRV_HISAX</th>\n",
       "      <th>CHARGER_BQ24190</th>\n",
       "      <th>SND_SOC_NAU8825</th>\n",
       "      <th>BH1750</th>\n",
       "      <th>NETWORK_FILESYSTEMS</th>\n",
       "      <th>active_options</th>\n",
       "      <th>perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1435</td>\n",
       "      <td>50222120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1382</td>\n",
       "      <td>16660024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1626</td>\n",
       "      <td>43080856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2140</td>\n",
       "      <td>27261672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 9469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X86_LOCAL_APIC  OPENVSWITCH  TEXTSEARCH_FSM  NETFILTER_XT_MATCH_TCPMSS  \\\n",
       "0               1            0               0                          0   \n",
       "1               1            0               0                          0   \n",
       "2               1            0               0                          0   \n",
       "3               1            0               0                          0   \n",
       "\n",
       "   MPLS  NFC_HCI  NETFILTER_XT_MATCH_TIME  NET_MPLS_GSO  NFC_SHDLC  \\\n",
       "0     1        0                        0             1          0   \n",
       "1     0        0                        0             0          0   \n",
       "2     0        0                        0             0          0   \n",
       "3     0        0                        0             0          0   \n",
       "\n",
       "   NETFILTER_XT_MATCH_U32  ...  ARCH_SUPPORTS_INT128  SLABINFO  MICROCODE_AMD  \\\n",
       "0                       0  ...                     1         0              0   \n",
       "1                       0  ...                     1         1              0   \n",
       "2                       0  ...                     1         0              1   \n",
       "3                       0  ...                     1         1              1   \n",
       "\n",
       "   ISDN_DRV_HISAX  CHARGER_BQ24190  SND_SOC_NAU8825  BH1750  \\\n",
       "0               0                1                0       0   \n",
       "1               0                0                0       0   \n",
       "2               0                0                0       0   \n",
       "3               0                1                0       1   \n",
       "\n",
       "   NETWORK_FILESYSTEMS  active_options      perf  \n",
       "0                    0            1435  50222120  \n",
       "1                    0            1382  16660024  \n",
       "2                    0            1626  43080856  \n",
       "3                    0            2140  27261672  \n",
       "\n",
       "[4 rows x 9469 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('data.parquet')\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92562 entries, 0 to 92561\n",
      "Columns: 9469 entries, X86_LOCAL_APIC to perf\n",
      "dtypes: int64(9469)\n",
      "memory usage: 6.5 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Float point transformation\n",
    "\n",
    "+ Pass the data to float\n",
    "+ Get used on pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X86_LOCAL_APIC</th>\n",
       "      <th>OPENVSWITCH</th>\n",
       "      <th>TEXTSEARCH_FSM</th>\n",
       "      <th>NETFILTER_XT_MATCH_TCPMSS</th>\n",
       "      <th>MPLS</th>\n",
       "      <th>NFC_HCI</th>\n",
       "      <th>NETFILTER_XT_MATCH_TIME</th>\n",
       "      <th>NET_MPLS_GSO</th>\n",
       "      <th>NFC_SHDLC</th>\n",
       "      <th>NETFILTER_XT_MATCH_U32</th>\n",
       "      <th>...</th>\n",
       "      <th>ARCH_SUPPORTS_INT128</th>\n",
       "      <th>SLABINFO</th>\n",
       "      <th>MICROCODE_AMD</th>\n",
       "      <th>ISDN_DRV_HISAX</th>\n",
       "      <th>CHARGER_BQ24190</th>\n",
       "      <th>SND_SOC_NAU8825</th>\n",
       "      <th>BH1750</th>\n",
       "      <th>NETWORK_FILESYSTEMS</th>\n",
       "      <th>active_options</th>\n",
       "      <th>perf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2368.0</td>\n",
       "      <td>187356352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17569</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>22655192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>23184328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1423.0</td>\n",
       "      <td>18807096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61009</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1421.0</td>\n",
       "      <td>19454496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45088</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>43939520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>29591920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23131</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1822.0</td>\n",
       "      <td>47054048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51233</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>33176640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35192</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>291209248.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 9469 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X86_LOCAL_APIC  OPENVSWITCH  TEXTSEARCH_FSM  NETFILTER_XT_MATCH_TCPMSS  \\\n",
       "8398              1.0          0.0             0.0                        0.0   \n",
       "17569             1.0          0.0             0.0                        1.0   \n",
       "7609              1.0          0.0             0.0                        0.0   \n",
       "19975             1.0          0.0             0.0                        0.0   \n",
       "61009             1.0          0.0             0.0                        0.0   \n",
       "...               ...          ...             ...                        ...   \n",
       "45088             1.0          0.0             0.0                        0.0   \n",
       "5260              1.0          0.0             0.0                        0.0   \n",
       "23131             1.0          0.0             0.0                        0.0   \n",
       "51233             1.0          0.0             0.0                        0.0   \n",
       "35192             1.0          0.0             0.0                        0.0   \n",
       "\n",
       "       MPLS  NFC_HCI  NETFILTER_XT_MATCH_TIME  NET_MPLS_GSO  NFC_SHDLC  \\\n",
       "8398    0.0      0.0                      0.0           0.0        0.0   \n",
       "17569   1.0      0.0                      1.0           0.0        0.0   \n",
       "7609    0.0      0.0                      0.0           0.0        0.0   \n",
       "19975   0.0      0.0                      0.0           0.0        0.0   \n",
       "61009   0.0      0.0                      0.0           0.0        0.0   \n",
       "...     ...      ...                      ...           ...        ...   \n",
       "45088   0.0      0.0                      0.0           0.0        0.0   \n",
       "5260    0.0      0.0                      0.0           0.0        0.0   \n",
       "23131   1.0      0.0                      0.0           0.0        0.0   \n",
       "51233   0.0      0.0                      0.0           0.0        0.0   \n",
       "35192   0.0      0.0                      0.0           0.0        0.0   \n",
       "\n",
       "       NETFILTER_XT_MATCH_U32  ...  ARCH_SUPPORTS_INT128  SLABINFO  \\\n",
       "8398                      0.0  ...                   1.0       0.0   \n",
       "17569                     1.0  ...                   1.0       1.0   \n",
       "7609                      0.0  ...                   1.0       1.0   \n",
       "19975                     0.0  ...                   1.0       0.0   \n",
       "61009                     0.0  ...                   1.0       0.0   \n",
       "...                       ...  ...                   ...       ...   \n",
       "45088                     0.0  ...                   1.0       1.0   \n",
       "5260                      0.0  ...                   1.0       0.0   \n",
       "23131                     0.0  ...                   1.0       1.0   \n",
       "51233                     0.0  ...                   1.0       1.0   \n",
       "35192                     0.0  ...                   1.0       0.0   \n",
       "\n",
       "       MICROCODE_AMD  ISDN_DRV_HISAX  CHARGER_BQ24190  SND_SOC_NAU8825  \\\n",
       "8398             0.0             0.0              0.0              0.0   \n",
       "17569            1.0             0.0              0.0              0.0   \n",
       "7609             0.0             0.0              0.0              0.0   \n",
       "19975            0.0             0.0              0.0              0.0   \n",
       "61009            0.0             0.0              0.0              0.0   \n",
       "...              ...             ...              ...              ...   \n",
       "45088            1.0             0.0              0.0              0.0   \n",
       "5260             0.0             0.0              1.0              0.0   \n",
       "23131            0.0             0.0              1.0              0.0   \n",
       "51233            0.0             0.0              1.0              0.0   \n",
       "35192            0.0             0.0              0.0              0.0   \n",
       "\n",
       "       BH1750  NETWORK_FILESYSTEMS  active_options         perf  \n",
       "8398      1.0                  0.0          2368.0  187356352.0  \n",
       "17569     0.0                  0.0          1766.0   22655192.0  \n",
       "7609      0.0                  0.0          1210.0   23184328.0  \n",
       "19975     0.0                  0.0          1423.0   18807096.0  \n",
       "61009     0.0                  0.0          1421.0   19454496.0  \n",
       "...       ...                  ...             ...          ...  \n",
       "45088     1.0                  0.0          1436.0   43939520.0  \n",
       "5260      1.0                  0.0          1902.0   29591920.0  \n",
       "23131     0.0                  0.0          1822.0   47054048.0  \n",
       "51233     0.0                  0.0          2388.0   33176640.0  \n",
       "35192     1.0                  0.0          1878.0  291209248.0  \n",
       "\n",
       "[500 rows x 9469 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(500)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'perf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[target_column]\n",
    "X = df.drop(columns=[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:  (500, 9468) \n",
      " Y Shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X Shape: \",X.shape,\"\\n Y Shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pytorch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the models, using pytorch lightningmodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sequence_high_dropout = torch.nn.Sequential(nn.Linear(number_of_features, number_of_features/2),\n",
    "#                                      nn.ReLU(),\n",
    "#                                      nn.Dropout(0.8),\n",
    "#                                      nn.Linear(number_of_features/2, number_of_features/2),\n",
    "#                                      nn.ReLU(),\n",
    "#                                      nn.Dropout(0.5),\n",
    "#                                      nn.Linear(number_of_features/2, 1))\n",
    "\n",
    "# model_sequence_low_dropout = torch.nn.Sequential(nn.Linear(number_of_features, number_of_features/2),\n",
    "#                                      nn.ReLU(),\n",
    "#                                      nn.Dropout(0.3),\n",
    "#                                      nn.Linear(number_of_features/2, number_of_features/2),\n",
    "#                                      nn.ReLU(),\n",
    "#                                      nn.Dropout(0.3),\n",
    "#                                      nn.Linear(number_of_features/2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, num_features, activation=\"ReLU\", optimizer_name=\"Adam\", loss_name=\"MSELoss\"):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.activation = activation\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.loss_name = loss_name\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self(x)\n",
    "        loss = self.get_loss_function()(z, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self(x)\n",
    "        loss = self.get_loss_function()(z, x)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.get_optimizer()\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        optimizers = {\n",
    "            \"Adam\": torch.optim.Adam(self.parameters(), lr=0.001),\n",
    "            \"SGD\": torch.optim.SGD(self.parameters(), lr=0.001),\n",
    "            \"RMSprop\": torch.optim.RMSprop(self.parameters(), lr=0.001)\n",
    "        }\n",
    "        return optimizers[self.optimizer_name]\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        loss_functions = {\n",
    "            \"MSELoss\": nn.MSELoss(),\n",
    "            \"L1Loss\": nn.L1Loss(),\n",
    "            \"SmoothL1Loss\": nn.SmoothL1Loss(),\n",
    "            \"CrossEntropyLoss\": nn.CrossEntropyLoss()\n",
    "        }\n",
    "        return loss_functions[self.loss_name]\n",
    "\n",
    "    def get_activation(self):\n",
    "        activations = {\n",
    "            \"ReLU\": nn.ReLU(),\n",
    "            \"PReLU\": nn.PReLU(),\n",
    "            \"ELU\": nn.ELU()\n",
    "        }\n",
    "        return activations[self.activation]\n",
    "\n",
    "    def build_model(self):\n",
    "        hidden_size = self.num_features // 2\n",
    "        hidden_size2 = hidden_size // 2\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.num_features, hidden_size),\n",
    "            self.get_activation(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size2),\n",
    "            self.get_activation(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size2, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the max epochs if your upper limmit is greater than 100 <br>\n",
    "`def train_model_tune(config, num_features, train_dataloader, val_dataloader, max_epochs=100):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_tune(config, num_features, train_dataloader, val_dataloader, max_epochs=100):\n",
    "    model = LightningModel(\n",
    "        num_features=num_features,\n",
    "        activation=config[\"activation\"],\n",
    "        optimizer_name=config[\"optimizer\"],\n",
    "        loss_name=config[\"loss_function\"]\n",
    "    )\n",
    "\n",
    "    metrics = {\"loss\": \"val_loss\"}\n",
    "    callbacks = [TuneReportCallback(metrics, on=\"validation_end\")]\n",
    "    \n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        callbacks=callbacks,\n",
    "        enable_progress_bar=False  # Disable progress bar for cleaner Ray Tune output\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(num_features, train_dataloader, val_dataloader, num_samples=10):\n",
    "    # Define the search space\n",
    "    config = {\n",
    "        \"optimizer\": tune.choice([\"Adam\", \"AdamW\"]),\n",
    "        \"loss_function\": tune.choice([\"MAPE\", \"MSE\",\"SmoothL1Loss\"]),\n",
    "        \"activation\": tune.choice([\"ReLU\", \"PReLU\", \"ELU\"]),\n",
    "    }\n",
    "    \n",
    "    search_algo = OptunaSearch(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr='training_iteration',\n",
    "        max_t=30,\n",
    "        grace_period=10,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            partial(\n",
    "                train_model_tune,\n",
    "                num_features=num_features,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader\n",
    "            ),\n",
    "            resources={\"cpu\": 1, \"gpu\": 0.5}  # Adjust based on your hardware\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            search_alg=search_algo,\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples\n",
    "        ),\n",
    "        param_space=config\n",
    "    )\n",
    "    \n",
    "    results = tuner.fit()\n",
    "    \n",
    "    # Get best trial\n",
    "    best_result = results.get_best_result(metric=\"loss\", mode=\"min\")\n",
    "    best_trial_config = best_result.config\n",
    "    best_trial_loss = best_result.metrics['loss']\n",
    "    print(f\"Best trial config: {best_trial_config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial_loss}\")\n",
    "    \n",
    "    return best_trial_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9468"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = X.shape[1]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-01-21 21:28:49</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:40.96        </td></tr>\n",
       "<tr><td>Memory:      </td><td>26.4/30.5 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 20.000: -2.2465365827083588 | Iter 10.000: -3.936624675989151<br>Logical resource usage: 2.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 3<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_tune_b13f0445</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-01-21_21-28-05_447780_244198/artifacts/2025-01-21_21-28-08/train_model_tune_2025-01-21_21-28-05/driver_artifacts/train_model_tune_b13f0445_1_activation=ELU,loss_function=SmoothL1Loss,optimizer=AdamW_2025-01-21_21-28-08/error.txt</td></tr>\n",
       "<tr><td>train_model_tune_d175faa3</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-01-21_21-28-05_447780_244198/artifacts/2025-01-21_21-28-08/train_model_tune_2025-01-21_21-28-05/driver_artifacts/train_model_tune_d175faa3_3_activation=ReLU,loss_function=MSE,optimizer=Adam_2025-01-21_21-28-13/error.txt         </td></tr>\n",
       "<tr><td>train_model_tune_71d4ebfc</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-01-21_21-28-05_447780_244198/artifacts/2025-01-21_21-28-08/train_model_tune_2025-01-21_21-28-05/driver_artifacts/train_model_tune_71d4ebfc_4_activation=ELU,loss_function=MAPE,optimizer=AdamW_2025-01-21_21-28-15/error.txt        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc                 </th><th>activation  </th><th>loss_function  </th><th>optimizer  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_tune_fe544ee3</td><td>RUNNING </td><td>192.168.0.176:246005</td><td>PReLU       </td><td>SmoothL1Loss   </td><td>Adam       </td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         34.5949</td><td style=\"text-align: right;\">3.11031 </td></tr>\n",
       "<tr><td>train_model_tune_8f4cd759</td><td>RUNNING </td><td>192.168.0.176:246324</td><td>ReLU        </td><td>SmoothL1Loss   </td><td>Adam       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         27.2251</td><td style=\"text-align: right;\">0.258359</td></tr>\n",
       "<tr><td>train_model_tune_20913f54</td><td>PENDING </td><td>                    </td><td>ReLU        </td><td>MSE            </td><td>AdamW      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>train_model_tune_b13f0445</td><td>ERROR   </td><td>192.168.0.176:245922</td><td>ELU         </td><td>SmoothL1Loss   </td><td>AdamW      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>train_model_tune_d175faa3</td><td>ERROR   </td><td>192.168.0.176:246127</td><td>ReLU        </td><td>MSE            </td><td>Adam       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>train_model_tune_71d4ebfc</td><td>ERROR   </td><td>192.168.0.176:246236</td><td>ELU         </td><td>MAPE           </td><td>AdamW      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (18 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=245922)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(func pid=245922)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(func pid=245922)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(func pid=245922)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(func pid=245922)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(func pid=245922)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2025-01-21 21:28:11,625\tERROR tune_controller.py:1331 -- Trial task failed for trial train_model_tune_b13f0445\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=245922, ip=192.168.0.176, actor_id=f3b052ba9688fca5ae500bb101000000, repr=func)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_244198/4030632213.py\", line 20, in train_model_tune\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 958, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py\", line 159, in setup\n",
      "    self.setup_optimizers(trainer)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py\", line 139, in setup_optimizers\n",
      "    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py\", line 180, in _init_optimizers_and_lr_schedulers\n",
      "    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_244198/401760164.py\", line 30, in configure_optimizers\n",
      "  File \"/tmp/ipykernel_244198/401760164.py\", line 38, in get_optimizer\n",
      "KeyError: 'AdamW'\n",
      "\u001b[36m(func pid=246005)\u001b[0m \n",
      "\u001b[36m(func pid=246005)\u001b[0m   | Name  | Type       | Params | Mode \n",
      "\u001b[36m(func pid=246005)\u001b[0m ---------------------------------------------\n",
      "\u001b[36m(func pid=246005)\u001b[0m 0 | model | Sequential | 56.0 M | train\n",
      "\u001b[36m(func pid=246005)\u001b[0m ---------------------------------------------\n",
      "\u001b[36m(func pid=246005)\u001b[0m 56.0 M    Trainable params\n",
      "\u001b[36m(func pid=246005)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(func pid=246005)\u001b[0m 56.0 M    Total params\n",
      "\u001b[36m(func pid=246005)\u001b[0m 224.145   Total estimated model params size (MB)\n",
      "\u001b[36m(func pid=246005)\u001b[0m 8         Modules in train mode\n",
      "\u001b[36m(func pid=246005)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(func pid=246005)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(func pid=246005)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([100, 9468])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(func pid=246005)\u001b[0m   return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "\u001b[36m(func pid=246005)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(func pid=246005)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(func pid=246005)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([400, 9468])) that is different to the input size (torch.Size([400, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(func pid=246005)\u001b[0m   return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "\u001b[36m(func pid=246127)\u001b[0m \n",
      "2025-01-21 21:28:16,438\tERROR tune_controller.py:1331 -- Trial task failed for trial train_model_tune_d175faa3\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=246127, ip=192.168.0.176, actor_id=57e281157f4c0c53a6d2766c01000000, repr=func)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_244198/4030632213.py\", line 20, in train_model_tune\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 144, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 433, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 323, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_244198/401760164.py\", line 25, in validation_step\n",
      "  File \"/tmp/ipykernel_244198/401760164.py\", line 47, in get_loss_function\n",
      "KeyError: 'MSE'\n",
      "\u001b[36m(func pid=246236)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(func pid=246236)\u001b[0m GPU available: True (cuda), used: True\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(func pid=246236)\u001b[0m TPU available: False, using: 0 TPU cores\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(func pid=246236)\u001b[0m HPU available: False, using: 0 HPUs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(func pid=246236)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(func pid=246236)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(func pid=246127)\u001b[0m   | Name  | Type       | Params | Mode \n",
      "\u001b[36m(func pid=246127)\u001b[0m ---------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(func pid=246127)\u001b[0m 0 | model | Sequential | 56.0 M | train\n",
      "\u001b[36m(func pid=246127)\u001b[0m 56.0 M    Trainable params\n",
      "\u001b[36m(func pid=246127)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(func pid=246127)\u001b[0m 56.0 M    Total params\n",
      "\u001b[36m(func pid=246127)\u001b[0m 224.145   Total estimated model params size (MB)\n",
      "\u001b[36m(func pid=246127)\u001b[0m 8         Modules in train mode\n",
      "\u001b[36m(func pid=246127)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(func pid=246127)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "2025-01-21 21:28:19,773\tERROR tune_controller.py:1331 -- Trial task failed for trial train_model_tune_71d4ebfc\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(KeyError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=246236, ip=192.168.0.176, actor_id=8ad943ce89677d737f18257501000000, repr=func)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_244198/4030632213.py\", line 20, in train_model_tune\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 958, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py\", line 159, in setup\n",
      "    self.setup_optimizers(trainer)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py\", line 139, in setup_optimizers\n",
      "    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py\", line 180, in _init_optimizers_and_lr_schedulers\n",
      "    optim_conf = call._call_lightning_module_hook(model.trainer, \"configure_optimizers\", pl_module=model)\n",
      "  File \"/home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 171, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_244198/401760164.py\", line 30, in configure_optimizers\n",
      "  File \"/tmp/ipykernel_244198/401760164.py\", line 38, in get_optimizer\n",
      "KeyError: 'AdamW'\n",
      "\u001b[36m(func pid=246324)\u001b[0m \n",
      "\u001b[36m(func pid=246324)\u001b[0m   | Name  | Type       | Params | Mode \n",
      "\u001b[36m(func pid=246324)\u001b[0m 0 | model | Sequential | 56.0 M | train\n",
      "\u001b[36m(func pid=246324)\u001b[0m 56.0 M    Trainable params\n",
      "\u001b[36m(func pid=246324)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(func pid=246324)\u001b[0m 56.0 M    Total params\n",
      "\u001b[36m(func pid=246324)\u001b[0m 224.145   Total estimated model params size (MB)\n",
      "\u001b[36m(func pid=246324)\u001b[0m 8         Modules in train mode\n",
      "\u001b[36m(func pid=246324)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(func pid=246324)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(func pid=246324)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([100, 9468])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(func pid=246324)\u001b[0m   return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "\u001b[36m(func pid=246324)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(func pid=246324)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(func pid=246324)\u001b[0m /home/pedro/Documents/dl4spl2/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([400, 9468])) that is different to the input size (torch.Size([400, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(func pid=246324)\u001b[0m   return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "2025-01-21 21:28:49,454\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-01-21 21:28:49,459\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/pedro/ray_results/train_model_tune_2025-01-21_21-28-05' in 0.0025s.\n",
      "2025-01-21 21:28:50,712\tERROR tune.py:1037 -- Trials did not complete: [train_model_tune_b13f0445, train_model_tune_d175faa3, train_model_tune_71d4ebfc]\n",
      "2025-01-21 21:28:50,713\tINFO tune.py:1041 -- Total run time: 42.40 seconds (40.96 seconds for the tuning loop).\n",
      "2025-01-21 21:28:50,713\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/pedro/ray_results/train_model_tune_2025-01-21_21-28-05\", trainable=...)\n",
      "2025-01-21 21:28:50,722\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- train_model_tune_20913f54: FileNotFoundError('Could not fetch metrics for train_model_tune_20913f54: both result.json and progress.csv were not found at /home/pedro/ray_results/train_model_tune_2025-01-21_21-28-05/train_model_tune_20913f54_6_activation=ReLU,loss_function=MSE,optimizer=AdamW_2025-01-21_21-28-22')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'optimizer': 'Adam', 'loss_function': 'SmoothL1Loss', 'activation': 'ReLU'}\n",
      "Best trial final validation loss: 0.2583586573600769\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LightningModel.__init__() got an unexpected keyword argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m best_config \u001b[38;5;241m=\u001b[39m tune_hyperparameters(\n\u001b[1;32m      2\u001b[0m         num_features\u001b[38;5;241m=\u001b[39mnum_features,\n\u001b[1;32m      3\u001b[0m         train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m      4\u001b[0m         val_dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m      5\u001b[0m         num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of trials to run\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0m final_model \u001b[38;5;241m=\u001b[39m \u001b[43mLightningModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbest_config\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m final_trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     14\u001b[0m         max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     15\u001b[0m         accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m         devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m final_trainer\u001b[38;5;241m.\u001b[39mfit(final_model, train_dataloader, test_dataloader)\n",
      "\u001b[0;31mTypeError\u001b[0m: LightningModel.__init__() got an unexpected keyword argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "best_config = tune_hyperparameters(\n",
    "        num_features=num_features,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=test_dataloader,\n",
    "        num_samples=10  # Number of trials to run\n",
    "    )\n",
    "\n",
    "final_model = LightningModel(\n",
    "        num_features=num_features,\n",
    "        **best_config\n",
    "    )\n",
    "\n",
    "final_trainer = L.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='auto',\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "final_trainer.fit(final_model, train_dataloader, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
